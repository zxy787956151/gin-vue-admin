# gin-vue-admin + Qwen3 + vLLM æœ¬åœ°éƒ¨ç½²æ–¹æ¡ˆ

## ğŸ“‹ ç›®å½•

- [1. æ–¹æ¡ˆæ¦‚è¿°](#1-æ–¹æ¡ˆæ¦‚è¿°)
- [2. ç³»ç»Ÿæ¶æ„](#2-ç³»ç»Ÿæ¶æ„)
- [3. ç¯å¢ƒè¦æ±‚](#3-ç¯å¢ƒè¦æ±‚)
- [4. æŠ€æœ¯æ ˆ](#4-æŠ€æœ¯æ ˆ)
- [5. éƒ¨ç½²æ­¥éª¤](#5-éƒ¨ç½²æ­¥éª¤)
- [6. é›†æˆæ–¹æ¡ˆ](#6-é›†æˆæ–¹æ¡ˆ)
- [7. API è®¾è®¡](#7-api-è®¾è®¡)
- [8. æ€§èƒ½ä¼˜åŒ–](#8-æ€§èƒ½ä¼˜åŒ–)
- [9. ç›‘æ§è¿ç»´](#9-ç›‘æ§è¿ç»´)
- [10. å¸¸è§é—®é¢˜](#10-å¸¸è§é—®é¢˜)

---

## 1. æ–¹æ¡ˆæ¦‚è¿°

### 1.1 ç›®æ ‡

åœ¨æœ¬åœ°æœåŠ¡å™¨ä¸Šéƒ¨ç½² Qwen3 å¤§è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨ vLLM ä½œä¸ºæ¨ç†å¼•æ“ï¼Œé€šè¿‡ gin-vue-admin æä¾› Web API æœåŠ¡ã€‚

### 1.2 æ¶æ„æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ç”¨æˆ·å±‚                                   â”‚
â”‚         Webæµè§ˆå™¨ / ç§»åŠ¨ç«¯ / APIè°ƒç”¨                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Nginx (å¯é€‰)                               â”‚
â”‚              åå‘ä»£ç† / è´Ÿè½½å‡è¡¡ / SSL                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 gin-vue-admin (Go)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  API è·¯ç”±å±‚                                           â”‚   â”‚
â”‚  â”‚  - /qianwen/chat (é€šä¹‰åƒé—®äº‘ç«¯)                       â”‚   â”‚
â”‚  â”‚  - /localai/chat (æœ¬åœ°Qwen3 + vLLM)                 â”‚   â”‚
â”‚  â”‚  - /lottery/check (å½©ç¥¨æ£€æŸ¥)                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  æœåŠ¡å±‚ (Service)                                     â”‚   â”‚
â”‚  â”‚  - è¯·æ±‚é¢„å¤„ç†                                         â”‚   â”‚
â”‚  â”‚  - å‚æ•°éªŒè¯                                           â”‚   â”‚
â”‚  â”‚  - ç»“æœåå¤„ç†                                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   HTTP/gRPC è°ƒç”¨       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    vLLM æ¨ç†æœåŠ¡                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  OpenAI-Compatible API Server                        â”‚   â”‚
â”‚  â”‚  - /v1/chat/completions (å…¼å®¹OpenAIæ ¼å¼)            â”‚   â”‚
â”‚  â”‚  - /v1/completions                                   â”‚   â”‚
â”‚  â”‚  - /v1/models                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                     â”‚                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  PagedAttention å†…å­˜ç®¡ç†                             â”‚   â”‚
â”‚  â”‚  Continuous Batching æ‰¹å¤„ç†                          â”‚   â”‚
â”‚  â”‚  Tensor Parallelism å¹¶è¡Œæ¨ç†                         â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Qwen3 æ¨¡å‹æ–‡ä»¶                               â”‚
â”‚  - qwen2.5-7b-instruct (7Bå‚æ•°, çº¦14GB)                     â”‚
â”‚  - qwen2.5-14b-instruct (14Bå‚æ•°, çº¦28GB)                   â”‚
â”‚  - qwen2.5-32b-instruct (32Bå‚æ•°, çº¦64GB)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ ¸å¿ƒä¼˜åŠ¿

âœ… **é«˜æ€§èƒ½**: vLLM æä¾›ä¸šç•Œé¢†å…ˆçš„æ¨ç†æ€§èƒ½  
âœ… **å…¼å®¹æ€§**: OpenAI API æ ¼å¼ï¼Œæ˜“äºè¿ç§»  
âœ… **çµæ´»æ€§**: æ”¯æŒå¤šç§æ¨¡å‹è§„æ ¼  
âœ… **å¯æ‰©å±•**: æ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²  
âœ… **ä½å»¶è¿Ÿ**: Continuous Batching ä¼˜åŒ–  

---

## 2. ç³»ç»Ÿæ¶æ„

### 2.1 æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        åº”ç”¨å±‚                                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  gin-vue-admin                                               â”‚
â”‚  - API Gateway                                               â”‚
â”‚  - è¯·æ±‚è·¯ç”±                                                   â”‚
â”‚  - é‰´æƒè®¤è¯                                                   â”‚
â”‚  - è¯·æ±‚é™æµ                                                   â”‚
â”‚  - æ—¥å¿—è®°å½•                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ¨ç†å±‚                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  vLLM æ¨ç†å¼•æ“                                               â”‚
â”‚  - PagedAttention (å†…å­˜ä¼˜åŒ–)                                â”‚
â”‚  - Continuous Batching (ååé‡ä¼˜åŒ–)                         â”‚
â”‚  - Tensor Parallelism (å¤šGPUå¹¶è¡Œ)                           â”‚
â”‚  - Quantization (é‡åŒ–æ”¯æŒ)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ¨¡å‹å±‚                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Qwen3 ç³»åˆ—æ¨¡å‹                                              â”‚
â”‚  - Transformer æ¶æ„                                          â”‚
â”‚  - å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶                                             â”‚
â”‚  - ä½ç½®ç¼–ç                                                    â”‚
â”‚  - Layer Normalization                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 æ•°æ®æµ

```
ç”¨æˆ·è¯·æ±‚
    â”‚
    â”œâ”€> gin-vue-admin æ¥æ”¶è¯·æ±‚
    â”‚       â”‚
    â”‚       â”œâ”€> å‚æ•°éªŒè¯
    â”‚       â”œâ”€> æ ¼å¼è½¬æ¢
    â”‚       â””â”€> è°ƒç”¨ vLLM API
    â”‚
    â”œâ”€> vLLM å¤„ç†è¯·æ±‚
    â”‚       â”‚
    â”‚       â”œâ”€> è¯·æ±‚æ’é˜Ÿ
    â”‚       â”œâ”€> Tokenization (åˆ†è¯)
    â”‚       â”œâ”€> æ‰¹å¤„ç†ç»„è£…
    â”‚       â”œâ”€> GPU æ¨ç†
    â”‚       â”‚   â”œâ”€> Forward Pass
    â”‚       â”‚   â”œâ”€> Attention è®¡ç®—
    â”‚       â”‚   â””â”€> Token ç”Ÿæˆ
    â”‚       â””â”€> Detokenization (è§£ç )
    â”‚
    â””â”€> è¿”å›ç»“æœ
            â”‚
            â”œâ”€> æ ¼å¼åŒ–è¾“å‡º
            â”œâ”€> æ—¥å¿—è®°å½•
            â””â”€> è¿”å›ç»™ç”¨æˆ·
```

---

## 3. ç¯å¢ƒè¦æ±‚

### 3.1 ç¡¬ä»¶è¦æ±‚

#### æœ€ä½é…ç½®ï¼ˆQwen2.5-7Bï¼‰
- **CPU**: 8æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**: 32GB RAM
- **GPU**: NVIDIA GPU with 16GB+ VRAM
  - æ¨è: RTX 4090 (24GB)
  - æˆ–: A10 (24GB)
  - æˆ–: V100 (16GB)
- **å­˜å‚¨**: 100GB å¯ç”¨ç©ºé—´
  - æ¨¡å‹æ–‡ä»¶: ~14GB
  - ç³»ç»Ÿä¾èµ–: ~10GB
  - ç¼“å­˜ç©ºé—´: ~30GB
  - æ—¥å¿—å¤‡ä»½: ~20GB

#### æ¨èé…ç½®ï¼ˆQwen2.5-14Bï¼‰
- **CPU**: 16æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**: 64GB RAM
- **GPU**: NVIDIA GPU with 32GB+ VRAM
  - æ¨è: RTX 4090 x2
  - æˆ–: A100 (40GB/80GB)
  - æˆ–: H100 (80GB)
- **å­˜å‚¨**: 200GB å¯ç”¨ç©ºé—´

#### é«˜æ€§èƒ½é…ç½®ï¼ˆQwen2.5-32Bï¼‰
- **CPU**: 32æ ¸å¿ƒä»¥ä¸Š
- **å†…å­˜**: 128GB RAM
- **GPU**: NVIDIA A100/H100 x2 æˆ–æ›´å¤š
- **å­˜å‚¨**: 500GB+ NVMe SSD

### 3.2 è½¯ä»¶è¦æ±‚

#### æ“ä½œç³»ç»Ÿ
- **Ubuntu**: 20.04 / 22.04 LTSï¼ˆæ¨èï¼‰
- **CentOS**: 7.x / 8.x
- **Rocky Linux**: 8.x / 9.x

#### CUDA ç¯å¢ƒ
- **CUDA**: 11.8 / 12.1 / 12.2
- **cuDNN**: 8.x
- **NVIDIA Driver**: 525.x æˆ–æ›´é«˜

#### Python ç¯å¢ƒ
- **Python**: 3.9 / 3.10 / 3.11ï¼ˆæ¨è 3.10ï¼‰
- **pip**: æœ€æ–°ç‰ˆæœ¬

#### Go ç¯å¢ƒ
- **Go**: 1.20 / 1.21 / 1.22

### 3.3 ç½‘ç»œè¦æ±‚

- **å¸¦å®½**: 100Mbps ä»¥ä¸Š
- **æ¨¡å‹ä¸‹è½½**: éœ€è¦è®¿é—® Hugging Face æˆ–å›½å†…é•œåƒ
- **ç«¯å£å¼€æ”¾**:
  - 8000: vLLM æœåŠ¡ç«¯å£
  - 8888: gin-vue-admin æœåŠ¡ç«¯å£
  - 80/443: Nginxï¼ˆå¦‚ä½¿ç”¨ï¼‰

---

## 4. æŠ€æœ¯æ ˆ

### 4.1 åç«¯æŠ€æœ¯

| ç»„ä»¶ | ç‰ˆæœ¬ | è¯´æ˜ |
|------|------|------|
| Go | 1.21+ | gin-vue-admin åç«¯ |
| Gin | v1.9+ | Web æ¡†æ¶ |
| Python | 3.10 | vLLM è¿è¡Œç¯å¢ƒ |
| vLLM | 0.5.0+ | æ¨ç†å¼•æ“ |
| PyTorch | 2.1+ | æ·±åº¦å­¦ä¹ æ¡†æ¶ |
| CUDA | 12.1 | GPU åŠ é€Ÿ |

### 4.2 æ¨¡å‹é€‰æ‹©

| æ¨¡å‹ | å‚æ•°é‡ | æ˜¾å­˜éœ€æ±‚ | é€‚ç”¨åœºæ™¯ |
|------|--------|---------|---------|
| Qwen2.5-7B | 7B | 16GB | é€šç”¨å¯¹è¯ï¼Œå¼€å‘æµ‹è¯• |
| Qwen2.5-14B | 14B | 32GB | å¤æ‚ä»»åŠ¡ï¼Œç”Ÿäº§ç¯å¢ƒ |
| Qwen2.5-32B | 32B | 80GB | é«˜è´¨é‡è¾“å‡ºï¼Œä¸“ä¸šåœºæ™¯ |
| Qwen2.5-72B | 72B | 160GB+ | é¡¶çº§æ€§èƒ½ï¼ˆéœ€å¤šå¡ï¼‰ |

### 4.3 é‡åŒ–æ–¹æ¡ˆ

| é‡åŒ–æ–¹å¼ | ç²¾åº¦ | æ˜¾å­˜èŠ‚çœ | æ€§èƒ½å½±å“ |
|---------|------|---------|---------|
| FP16 | åŠç²¾åº¦ | 50% | æœ€å° |
| INT8 | 8ä½æ•´æ•° | 75% | è½»å¾® |
| INT4 | 4ä½æ•´æ•° | 87.5% | ä¸­ç­‰ |
| AWQ | è‡ªé€‚åº”æƒé‡é‡åŒ– | 75% | æœ€å° |
| GPTQ | é€šç”¨é‡åŒ– | 75% | è½»å¾® |

**æ¨è**: AWQ æˆ– GPTQï¼ˆæ€§èƒ½å’Œè´¨é‡å¹³è¡¡æœ€ä½³ï¼‰

---

## 5. éƒ¨ç½²æ­¥éª¤

### 5.1 ç¯å¢ƒå‡†å¤‡

#### æ­¥éª¤1: å®‰è£… CUDA å’Œé©±åŠ¨

```bash
# æ£€æŸ¥æ˜¾å¡
nvidia-smi

# å®‰è£… CUDA 12.1
wget https://developer.download.nvidia.com/compute/cuda/12.1.0/local_installers/cuda_12.1.0_530.30.02_linux.run
sudo sh cuda_12.1.0_530.30.02_linux.run

# è®¾ç½®ç¯å¢ƒå˜é‡
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# éªŒè¯
nvcc --version
```

#### æ­¥éª¤2: å®‰è£… Python ç¯å¢ƒ

```bash
# å®‰è£… Python 3.10
sudo apt update
sudo apt install python3.10 python3.10-venv python3.10-dev

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.10 -m venv vllm-env
source vllm-env/bin/activate

# å‡çº§ pip
pip install --upgrade pip
```

#### æ­¥éª¤3: å®‰è£… vLLM

```bash
# æ–¹å¼1: ä» PyPI å®‰è£…ï¼ˆæ¨èï¼‰
pip install vllm

# æ–¹å¼2: ä»æºç å®‰è£…ï¼ˆæœ€æ–°ç‰ˆï¼‰
git clone https://github.com/vllm-project/vllm.git
cd vllm
pip install -e .

# éªŒè¯å®‰è£…
python -c "import vllm; print(vllm.__version__)"
```

### 5.2 æ¨¡å‹ä¸‹è½½

#### æ–¹å¼1: ä» Hugging Face ä¸‹è½½

```bash
# å®‰è£… huggingface-cli
pip install huggingface-hub

# ç™»å½•ï¼ˆå¯é€‰ï¼Œä¸‹è½½ gated modelsï¼‰
huggingface-cli login

# ä¸‹è½½ Qwen2.5-7B-Instruct
huggingface-cli download Qwen/Qwen2.5-7B-Instruct \
  --local-dir ./models/Qwen2.5-7B-Instruct \
  --local-dir-use-symlinks False

# ä¸‹è½½ AWQ é‡åŒ–ç‰ˆæœ¬ï¼ˆæ¨èï¼‰
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-AWQ \
  --local-dir ./models/Qwen2.5-7B-Instruct-AWQ \
  --local-dir-use-symlinks False
```

#### æ–¹å¼2: ä» ModelScope ä¸‹è½½ï¼ˆå›½å†…é•œåƒï¼‰

```bash
# å®‰è£… modelscope
pip install modelscope

# Python è„šæœ¬ä¸‹è½½
python << EOF
from modelscope import snapshot_download

model_dir = snapshot_download(
    'Qwen/Qwen2.5-7B-Instruct',
    cache_dir='./models'
)
print(f"æ¨¡å‹å·²ä¸‹è½½åˆ°: {model_dir}")
EOF
```

#### æ–¹å¼3: æ‰‹åŠ¨ä¸‹è½½

è®¿é—®ä»¥ä¸‹åœ°å€æ‰‹åŠ¨ä¸‹è½½ï¼š
- Hugging Face: https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
- ModelScope: https://modelscope.cn/models/Qwen/Qwen2.5-7B-Instruct

### 5.3 å¯åŠ¨ vLLM æœåŠ¡

#### åŸºç¡€å¯åŠ¨

```bash
# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source vllm-env/bin/activate

# å¯åŠ¨ vLLM (FP16)
python -m vllm.entrypoints.openai.api_server \
  --model ./models/Qwen2.5-7B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code
```

#### ä¼˜åŒ–å¯åŠ¨ï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨é‡åŒ–æ¨¡å‹ + ä¼˜åŒ–å‚æ•°
python -m vllm.entrypoints.openai.api_server \
  --model ./models/Qwen2.5-7B-Instruct-AWQ \
  --quantization awq \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.9 \
  --dtype auto \
  --tensor-parallel-size 1
```

#### å¤šGPUå¯åŠ¨

```bash
# ä½¿ç”¨ 2 å¼  GPU
python -m vllm.entrypoints.openai.api_server \
  --model ./models/Qwen2.5-14B-Instruct \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code \
  --tensor-parallel-size 2
```

#### åå°è¿è¡Œï¼ˆä½¿ç”¨ systemdï¼‰

åˆ›å»ºæœåŠ¡æ–‡ä»¶ `/etc/systemd/system/vllm.service`:

```ini
[Unit]
Description=vLLM OpenAI Compatible API Server
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/project
Environment="PATH=/path/to/vllm-env/bin:/usr/local/bin:/usr/bin"
ExecStart=/path/to/vllm-env/bin/python -m vllm.entrypoints.openai.api_server \
  --model /path/to/models/Qwen2.5-7B-Instruct-AWQ \
  --quantization awq \
  --host 0.0.0.0 \
  --port 8000 \
  --trust-remote-code \
  --max-model-len 4096 \
  --gpu-memory-utilization 0.9
Restart=on-failure
RestartSec=10s

[Install]
WantedBy=multi-user.target
```

å¯åŠ¨æœåŠ¡:

```bash
sudo systemctl daemon-reload
sudo systemctl enable vllm
sudo systemctl start vllm
sudo systemctl status vllm
```

### 5.4 éªŒè¯ vLLM æœåŠ¡

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/health

# åˆ—å‡ºå¯ç”¨æ¨¡å‹
curl http://localhost:8000/v1/models

# æµ‹è¯•å¯¹è¯
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct",
    "messages": [
      {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}
    ],
    "max_tokens": 500,
    "temperature": 0.7
  }'
```

---

## 6. é›†æˆæ–¹æ¡ˆ

### 6.1 gin-vue-admin é›†æˆæ¶æ„

```
gin-vue-admin
â”œâ”€â”€ config/
â”‚   â””â”€â”€ vllm.go              # vLLM é…ç½®
â”œâ”€â”€ service/
â”‚   â””â”€â”€ vllm/
â”‚       â”œâ”€â”€ client.go        # vLLM å®¢æˆ·ç«¯
â”‚       â”œâ”€â”€ chat.go          # å¯¹è¯æœåŠ¡
â”‚       â””â”€â”€ stream.go        # æµå¼è¾“å‡º
â”œâ”€â”€ api/v1/
â”‚   â””â”€â”€ vllm/
â”‚       â””â”€â”€ vllm.go          # API å¤„ç†å™¨
â””â”€â”€ router/
    â””â”€â”€ vllm/
        â””â”€â”€ vllm.go          # è·¯ç”±æ³¨å†Œ
```

### 6.2 é…ç½®è®¾è®¡

åœ¨ `config.yaml` ä¸­æ·»åŠ :

```yaml
# vLLM é…ç½®
vllm:
  enabled: true                      # æ˜¯å¦å¯ç”¨
  base-url: "http://localhost:8000"  # vLLM æœåŠ¡åœ°å€
  model: "Qwen2.5-7B-Instruct"       # æ¨¡å‹åç§°
  timeout: 300                       # è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰
  max-tokens: 2000                   # æœ€å¤§ç”Ÿæˆ token æ•°
  temperature: 0.7                   # æ¸©åº¦å‚æ•°
  top-p: 0.9                        # nucleus sampling
  stream: true                       # æ˜¯å¦æ”¯æŒæµå¼è¾“å‡º
```

### 6.3 API æ¥å£è®¾è®¡

#### æ¥å£1: æ ‡å‡†å¯¹è¯

```
POST /vllm/chat
```

**è¯·æ±‚**:
```json
{
  "message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
  "stream": false,
  "max_tokens": 2000,
  "temperature": 0.7
}
```

**å“åº”**:
```json
{
  "code": 0,
  "data": {
    "reply": "ä½ å¥½ï¼æˆ‘æ˜¯é€šä¹‰åƒé—®...",
    "usage": {
      "prompt_tokens": 10,
      "completion_tokens": 50,
      "total_tokens": 60
    },
    "model": "Qwen2.5-7B-Instruct",
    "time": "2.3s"
  },
  "msg": "è¯·æ±‚æˆåŠŸ"
}
```

#### æ¥å£2: å¤šè½®å¯¹è¯

```
POST /vllm/chat/multi
```

**è¯·æ±‚**:
```json
{
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„AIåŠ©æ‰‹"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯é‡å­è®¡ç®—ï¼Ÿ"},
    {"role": "assistant", "content": "é‡å­è®¡ç®—æ˜¯..."},
    {"role": "user", "content": "å®ƒæœ‰ä»€ä¹ˆåº”ç”¨ï¼Ÿ"}
  ],
  "stream": false
}
```

#### æ¥å£3: æµå¼è¾“å‡º

```
POST /vllm/chat/stream
```

**å“åº”**: Server-Sent Events (SSE)

```
data: {"content": "ä½ ", "done": false}
data: {"content": "å¥½", "done": false}
data: {"content": "ï¼", "done": false}
data: {"content": "", "done": true, "usage": {...}}
```

#### æ¥å£4: æ¨¡å‹ä¿¡æ¯

```
GET /vllm/models
```

**å“åº”**:
```json
{
  "code": 0,
  "data": {
    "models": [
      {
        "id": "Qwen2.5-7B-Instruct",
        "object": "model",
        "created": 1234567890,
        "owned_by": "Qwen"
      }
    ]
  }
}
```

### 6.4 é”™è¯¯å¤„ç†

```go
// é”™è¯¯ç å®šä¹‰
const (
    ErrVLLMUnavailable  = 5001 // vLLM æœåŠ¡ä¸å¯ç”¨
    ErrModelNotFound    = 5002 // æ¨¡å‹æœªæ‰¾åˆ°
    ErrTokenLimit       = 5003 // Token æ•°é‡è¶…é™
    ErrTimeout          = 5004 // è¯·æ±‚è¶…æ—¶
    ErrInvalidParams    = 5005 // å‚æ•°é”™è¯¯
)
```

---

## 7. API è®¾è®¡

### 7.1 ç»Ÿä¸€æ¥å£å±‚

```
/api/ai/
â”œâ”€â”€ /chat              # é€šç”¨å¯¹è¯æ¥å£ï¼ˆè‡ªåŠ¨é€‰æ‹©åç«¯ï¼‰
â”œâ”€â”€ /qianwen/chat      # é€šä¹‰åƒé—®äº‘ç«¯
â”œâ”€â”€ /vllm/chat         # æœ¬åœ° vLLM
â””â”€â”€ /localai/chat      # æœ¬åœ° Ollama/å…¶ä»–
```

### 7.2 è¯·æ±‚æ ¼å¼ç»Ÿä¸€

æ‰€æœ‰ AI æ¥å£ä½¿ç”¨ç»Ÿä¸€çš„è¯·æ±‚æ ¼å¼:

```json
{
  "message": "string",
  "history": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ],
  "options": {
    "max_tokens": 2000,
    "temperature": 0.7,
    "top_p": 0.9,
    "stream": false
  }
}
```

### 7.3 å“åº”æ ¼å¼ç»Ÿä¸€

```json
{
  "code": 0,
  "data": {
    "reply": "string",
    "model": "string",
    "usage": {
      "prompt_tokens": 0,
      "completion_tokens": 0,
      "total_tokens": 0
    },
    "time": "string"
  },
  "msg": "string"
}
```

---

## 8. æ€§èƒ½ä¼˜åŒ–

### 8.1 vLLM å‚æ•°ä¼˜åŒ–

```bash
# ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨
--gpu-memory-utilization 0.9    # ä½¿ç”¨ 90% GPU æ˜¾å­˜

# ä¼˜åŒ–æ‰¹å¤„ç†
--max-num-batched-tokens 8192   # æœ€å¤§æ‰¹å¤„ç† token æ•°
--max-num-seqs 256              # æœ€å¤§å¹¶å‘åºåˆ—æ•°

# ä¼˜åŒ–ä¸Šä¸‹æ–‡é•¿åº¦
--max-model-len 4096            # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦

# å¯ç”¨é‡åŒ–
--quantization awq              # ä½¿ç”¨ AWQ é‡åŒ–

# KV Cache ä¼˜åŒ–
--enable-prefix-caching         # å¯ç”¨å‰ç¼€ç¼“å­˜
--kv-cache-dtype auto           # è‡ªåŠ¨é€‰æ‹© KV Cache ç±»å‹
```

### 8.2 è¯·æ±‚ä¼˜åŒ–ç­–ç•¥

#### æ‰¹å¤„ç†ä¼˜åŒ–
```
åŒæ—¶å¤„ç†å¤šä¸ªè¯·æ±‚ï¼Œæé«˜ GPU åˆ©ç”¨ç‡
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Request 1: "ä½ å¥½..."           â”‚
â”‚  Request 2: "è§£é‡Šé‡å­..."       â”‚  â”€â”€â–¶  Batch Processing
â”‚  Request 3: "ç¿»è¯‘..."           â”‚       on GPU
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Continuous Batching
```
åŠ¨æ€æ‰¹å¤„ç†ï¼Œå‡å°‘ç­‰å¾…æ—¶é—´
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Time  â”‚ GPU Processing                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ T0    â”‚ Req1[0:10], Req2[0:10]         â”‚
â”‚ T1    â”‚ Req1[10:20], Req2[10:20]       â”‚
â”‚ T2    â”‚ Req1[20:30], Req3[0:10]  â—€â”€ æ–°è¯·æ±‚åŠ å…¥
â”‚ T3    â”‚ Req1[30:40], Req3[10:20]       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.3 gin-vue-admin ä¼˜åŒ–

#### è¿æ¥æ± 

```go
// HTTP è¿æ¥æ± é…ç½®
transport := &http.Transport{
    MaxIdleConns:        100,
    MaxIdleConnsPerHost: 100,
    IdleConnTimeout:     90 * time.Second,
}

client := &http.Client{
    Transport: transport,
    Timeout:   300 * time.Second,
}
```

#### è¯·æ±‚ç¼“å­˜

```go
// ç¼“å­˜ç›¸åŒçš„è¯·æ±‚ç»“æœ
cache := cache.New(5*time.Minute, 10*time.Minute)

if cached, found := cache.Get(requestHash); found {
    return cached.(Response)
}
```

#### é™æµæ§åˆ¶

```go
// ä»¤ç‰Œæ¡¶é™æµ
limiter := rate.NewLimiter(rate.Limit(10), 20) // æ¯ç§’10ä¸ªè¯·æ±‚ï¼Œæ¡¶å®¹é‡20
```

### 8.4 æ€§èƒ½ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| é¦–Tokenå»¶è¿Ÿ (TTFT) | < 500ms | ç”Ÿæˆç¬¬ä¸€ä¸ªtokençš„æ—¶é—´ |
| ç”Ÿæˆé€Ÿåº¦ (TPS) | > 50 tokens/s | æ¯ç§’ç”Ÿæˆçš„tokenæ•° |
| å¹¶å‘å¤„ç†èƒ½åŠ› | > 10 QPS | æ¯ç§’æŸ¥è¯¢æ•° |
| GPU åˆ©ç”¨ç‡ | > 80% | GPU ä½¿ç”¨æ•ˆç‡ |
| æ˜¾å­˜ä½¿ç”¨ç‡ | < 90% | é¿å… OOM |

---

## 9. ç›‘æ§è¿ç»´

### 9.1 æ—¥å¿—ç³»ç»Ÿ

#### vLLM æ—¥å¿—

```bash
# æŸ¥çœ‹ vLLM æ—¥å¿—
sudo journalctl -u vllm -f

# è®¾ç½®æ—¥å¿—çº§åˆ«
--log-level info  # debug, info, warning, error
```

#### gin-vue-admin æ—¥å¿—

```yaml
# config.yaml
zap:
  level: info
  format: json
  director: log/vllm
  show-line: true
```

### 9.2 æ€§èƒ½ç›‘æ§

#### Prometheus + Grafana

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'vllm'
    static_configs:
      - targets: ['localhost:8000']
    metrics_path: '/metrics'
```

ç›‘æ§æŒ‡æ ‡:
- `vllm_request_duration_seconds`: è¯·æ±‚å»¶è¿Ÿ
- `vllm_request_tokens_total`: Token ä½¿ç”¨é‡
- `vllm_gpu_memory_used`: GPU æ˜¾å­˜ä½¿ç”¨
- `vllm_num_requests_running`: æ­£åœ¨å¤„ç†çš„è¯·æ±‚æ•°

#### è‡ªå®šä¹‰ç›‘æ§è„šæœ¬

```bash
#!/bin/bash
# monitor_vllm.sh

while true; do
  # æ£€æŸ¥æœåŠ¡çŠ¶æ€
  if ! curl -s http://localhost:8000/health > /dev/null; then
    echo "$(date): vLLM æœåŠ¡å¼‚å¸¸" >> /var/log/vllm_monitor.log
    # å‘é€å‘Šè­¦
    # systemctl restart vllm
  fi
  
  # æ£€æŸ¥ GPU çŠ¶æ€
  nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total \
    --format=csv,noheader >> /var/log/gpu_stats.log
  
  sleep 60
done
```

### 9.3 å¤‡ä»½æ¢å¤

#### æ¨¡å‹å¤‡ä»½

```bash
# å¤‡ä»½æ¨¡å‹æ–‡ä»¶
tar -czf qwen2.5-7b-backup.tar.gz ./models/Qwen2.5-7B-Instruct/

# å¤‡ä»½é…ç½®
cp config.yaml config.yaml.backup
```

#### æ•°æ®åº“å¤‡ä»½

```bash
# å¤‡ä»½ gin-vue-admin æ•°æ®åº“
mysqldump -u root -p gva > gva_backup_$(date +%Y%m%d).sql
```

---

## 10. å¸¸è§é—®é¢˜

### 10.1 éƒ¨ç½²é—®é¢˜

#### Q1: CUDA ç‰ˆæœ¬ä¸åŒ¹é…

**é—®é¢˜**: `RuntimeError: CUDA error: no kernel image is available`

**è§£å†³**:
```bash
# é‡æ–°å®‰è£…åŒ¹é…çš„ PyTorch
pip uninstall torch
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

#### Q2: æ˜¾å­˜ä¸è¶³ (OOM)

**é—®é¢˜**: `RuntimeError: CUDA out of memory`

**è§£å†³æ–¹æ¡ˆ**:
1. ä½¿ç”¨é‡åŒ–æ¨¡å‹ (AWQ/GPTQ)
2. å‡å° `max-model-len`
3. é™ä½ `gpu-memory-utilization`
4. ä½¿ç”¨æ›´å°çš„æ¨¡å‹

```bash
# ä½¿ç”¨é‡åŒ–æ¨¡å‹
--quantization awq
--max-model-len 2048
--gpu-memory-utilization 0.85
```

#### Q3: vLLM å¯åŠ¨æ…¢

**é—®é¢˜**: å¯åŠ¨éœ€è¦å‡ åˆ†é’Ÿ

**åŸå› **: 
- æ¨¡å‹åŠ è½½éœ€è¦æ—¶é—´
- ç¼–è¯‘ CUDA kernels

**ä¼˜åŒ–**:
```bash
# é¢„ç¼–è¯‘
export VLLM_COMPILE_LEVEL=2
```

### 10.2 æ€§èƒ½é—®é¢˜

#### Q1: å“åº”é€Ÿåº¦æ…¢

**æ’æŸ¥**:
1. æ£€æŸ¥ GPU åˆ©ç”¨ç‡: `nvidia-smi`
2. æ£€æŸ¥å¹¶å‘æ•°: `--max-num-seqs`
3. æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿ: `ping localhost`

**ä¼˜åŒ–**:
```bash
--max-num-batched-tokens 8192
--max-num-seqs 128
--enable-prefix-caching
```

#### Q2: é¦–Tokenå»¶è¿Ÿé«˜

**åŸå› **:
- æ¨¡å‹å¤§å°
- Prompt é•¿åº¦
- GPU è´Ÿè½½

**ä¼˜åŒ–**:
- ä½¿ç”¨é‡åŒ–æ¨¡å‹
- ç¼©çŸ­ system prompt
- å¯ç”¨ prefix caching

### 10.3 é›†æˆé—®é¢˜

#### Q1: gin-vue-admin æ— æ³•è¿æ¥ vLLM

**æ£€æŸ¥**:
```bash
# æ£€æŸ¥ vLLM æœåŠ¡
curl http://localhost:8000/health

# æ£€æŸ¥é˜²ç«å¢™
sudo ufw status
sudo ufw allow 8000
```

#### Q2: è¯·æ±‚è¶…æ—¶

**é…ç½®**:
```yaml
# config.yaml
vllm:
  timeout: 300  # å¢åŠ è¶…æ—¶æ—¶é—´
```

```go
// Go ä»£ç 
client := &http.Client{
    Timeout: 300 * time.Second,
}
```

---

## 11. å®Œæ•´éƒ¨ç½²æ£€æŸ¥æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥

- [ ] ç¡¬ä»¶æ»¡è¶³æœ€ä½è¦æ±‚
- [ ] NVIDIA é©±åŠ¨å·²å®‰è£…
- [ ] CUDA ç¯å¢ƒå·²é…ç½®
- [ ] Python 3.10 å·²å®‰è£…
- [ ] Go 1.21+ å·²å®‰è£…
- [ ] ç½‘ç»œè®¿é—®æ­£å¸¸
- [ ] å­˜å‚¨ç©ºé—´å……è¶³

### vLLM éƒ¨ç½²æ£€æŸ¥

- [ ] vLLM å·²å®‰è£…
- [ ] æ¨¡å‹å·²ä¸‹è½½å®Œæˆ
- [ ] vLLM æœåŠ¡å¯åŠ¨æˆåŠŸ
- [ ] API æ¥å£æµ‹è¯•é€šè¿‡
- [ ] æ€§èƒ½æŒ‡æ ‡æ­£å¸¸
- [ ] systemd æœåŠ¡å·²é…ç½®
- [ ] å¼€æœºè‡ªå¯åŠ¨å·²è®¾ç½®

### gin-vue-admin é›†æˆæ£€æŸ¥

- [ ] é…ç½®æ–‡ä»¶å·²æ›´æ–°
- [ ] ä»£ç å·²ç¼–è¯‘é€šè¿‡
- [ ] API æ¥å£å·²å®ç°
- [ ] è·¯ç”±å·²æ³¨å†Œ
- [ ] é”™è¯¯å¤„ç†å·²å®Œå–„
- [ ] æ—¥å¿—è®°å½•å·²é…ç½®
- [ ] æµ‹è¯•ç”¨ä¾‹å·²é€šè¿‡

### ä¸Šçº¿å‰æ£€æŸ¥

- [ ] æ€§èƒ½æµ‹è¯•å·²å®Œæˆ
- [ ] å‹åŠ›æµ‹è¯•å·²é€šè¿‡
- [ ] ç›‘æ§ç³»ç»Ÿå·²éƒ¨ç½²
- [ ] å¤‡ä»½ç­–ç•¥å·²åˆ¶å®š
- [ ] åº”æ€¥é¢„æ¡ˆå·²å‡†å¤‡
- [ ] æ–‡æ¡£å·²æ›´æ–°

---

## 12. ä¸‹ä¸€æ­¥è®¡åˆ’

1. **ç¯å¢ƒå‡†å¤‡** (ç¬¬1å¤©)
   - å®‰è£… CUDA å’Œé©±åŠ¨
   - é…ç½® Python ç¯å¢ƒ
   - å®‰è£… vLLM

2. **æ¨¡å‹éƒ¨ç½²** (ç¬¬2-3å¤©)
   - ä¸‹è½½ Qwen2.5 æ¨¡å‹
   - å¯åŠ¨ vLLM æœåŠ¡
   - æ€§èƒ½æµ‹è¯•å’Œè°ƒä¼˜

3. **ä»£ç å¼€å‘** (ç¬¬4-5å¤©)
   - å®ç° gin-vue-admin é›†æˆ
   - å¼€å‘ API æ¥å£
   - ç¼–å†™æµ‹è¯•ç”¨ä¾‹

4. **æµ‹è¯•ä¸Šçº¿** (ç¬¬6-7å¤©)
   - åŠŸèƒ½æµ‹è¯•
   - æ€§èƒ½æµ‹è¯•
   - éƒ¨ç½²ä¸Šçº¿

---

## 13. å‚è€ƒèµ„æ–™

### å®˜æ–¹æ–‡æ¡£
- vLLM: https://docs.vllm.ai/
- Qwen: https://github.com/QwenLM/Qwen
- gin-vue-admin: https://www.gin-vue-admin.com/

### æ¨¡å‹ä¸‹è½½
- Hugging Face: https://huggingface.co/Qwen
- ModelScope: https://modelscope.cn/models/Qwen
- é­”æ­ç¤¾åŒº: https://www.modelscope.cn/

### ç¤¾åŒºèµ„æº
- vLLM Discord: https://discord.gg/vllm
- Qwen GitHub: https://github.com/QwenLM/Qwen

---

## é™„å½•

### A. ç¡¬ä»¶é€‰å‹å»ºè®®

#### å¼€å‘æµ‹è¯•ç¯å¢ƒ
- GPU: RTX 4090 (24GB) x1
- æˆæœ¬: ~12,000 RMB
- é€‚åˆ: Qwen2.5-7B

#### ç”Ÿäº§ç¯å¢ƒ
- GPU: NVIDIA A100 (40GB) x2
- æˆæœ¬: ~60,000 RMB
- é€‚åˆ: Qwen2.5-14B

#### é«˜æ€§èƒ½ç¯å¢ƒ
- GPU: NVIDIA H100 (80GB) x4
- æˆæœ¬: ~300,000+ RMB
- é€‚åˆ: Qwen2.5-72B

### B. æˆæœ¬ä¼°ç®—

| é¡¹ç›® | é…ç½® | æœˆæˆæœ¬ (äº‘æœåŠ¡) |
|------|------|----------------|
| å¼€å‘ç¯å¢ƒ | RTX 4090 x1 | ~2,000 RMB |
| ç”Ÿäº§ç¯å¢ƒ | A100 x2 | ~15,000 RMB |
| é«˜æ€§èƒ½ç¯å¢ƒ | H100 x4 | ~60,000+ RMB |

### C. æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | GPU | å¹¶å‘ | TPS | TTFT | é€‚ç”¨åœºæ™¯ |
|------|-----|------|-----|------|---------|
| 7B | 4090 x1 | 10 | 50 | 300ms | å¼€å‘æµ‹è¯• |
| 14B | A100 x2 | 20 | 80 | 400ms | ç”Ÿäº§ç¯å¢ƒ |
| 32B | H100 x4 | 30 | 100 | 500ms | é«˜ç«¯åœºæ™¯ |

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**æœ€åæ›´æ–°**: 2024-12-22  
**ä½œè€…**: AI Assistant  
**çŠ¶æ€**: å¾…å®æ–½

