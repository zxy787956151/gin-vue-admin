# vLLM éƒ¨ç½²æŒ‡å— - ç²¾ç®€å®ç”¨ç‰ˆ

## ğŸ“‹ ä½ çš„éƒ¨ç½²æ–¹æ¡ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      HTTP      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  macOS æœ¬åœ°              â”‚  â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º  â”‚  CentOS æœåŠ¡å™¨           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚                 â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ gin-vue-admin     â”‚  â”‚                 â”‚  â”‚ vLLM + Qwen3      â”‚  â”‚
â”‚  â”‚ docker-compose    â”‚  â”‚                 â”‚  â”‚ docker-compose    â”‚  â”‚
â”‚  â”‚ ç«¯å£: 8888        â”‚  â”‚                 â”‚  â”‚ ç«¯å£: 8000        â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚                 â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                 â”‚         GPU              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ç¬¬ä¸€æ­¥ï¼šCentOS æœåŠ¡å™¨éƒ¨ç½² vLLM

### 1.1 ç¯å¢ƒå‡†å¤‡

```bash
# SSH ç™»å½•æœåŠ¡å™¨
ssh root@your-centos-server

# æ£€æŸ¥ GPU
nvidia-smi

# å®‰è£… Dockerï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
curl -fsSL https://get.docker.com | bash
sudo systemctl start docker
sudo systemctl enable docker
```

### 1.2 å®‰è£… NVIDIA Container Toolkit

```bash
# æ–¹æ³•1ï¼šä½¿ç”¨é€šç”¨ RPM ä»“åº“ï¼ˆæ¨èï¼‰
curl -fsSL https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
  sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

sudo yum clean expire-cache
sudo yum install -y nvidia-container-toolkit

# é…ç½® Docker è¿è¡Œæ—¶
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

# éªŒè¯
docker run --rm --gpus all nvidia/cuda:12.0.0-base-centos7 nvidia-smi

# æ–¹æ³•2ï¼šå¦‚æœæ–¹æ³•1å¤±è´¥ï¼Œç›´æ¥å®‰è£…ï¼ˆCentOS 7/8ï¼‰
# sudo yum install -y yum-utils
# sudo yum-config-manager --add-repo https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
# sudo yum install -y nvidia-container-toolkit
# sudo nvidia-ctk runtime configure --runtime=docker
# sudo systemctl restart docker
```

### 1.3 åˆ›å»ºéƒ¨ç½²ç›®å½•

```bash
mkdir -p ~/vllm-deploy/models
cd ~/vllm-deploy
```

### 1.4 ä¸‹è½½æ¨¡å‹

```bash
# æ–¹å¼1: huggingface-cli
pip3 install huggingface-hub
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-AWQ \
  --local-dir ./models/Qwen2.5-7B-Instruct-AWQ

# æ–¹å¼2: ModelScopeï¼ˆå›½å†…å¿«ï¼‰
pip3 install modelscope
python3 << EOF
from modelscope import snapshot_download
snapshot_download('Qwen/Qwen2.5-7B-Instruct-AWQ', cache_dir='./models')
EOF
```

### 1.5 åˆ›å»º docker-compose.yml

```yaml
version: '3.8'

services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm-qwen
    restart: unless-stopped
    runtime: nvidia
    
    ports:
      - "8000:8000"
    
    volumes:
      - ./models:/models:ro
      - ./cache:/root/.cache
    
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    
    command: >
      --model /models/Qwen2.5-7B-Instruct-AWQ
      --quantization awq
      --host 0.0.0.0
      --port 8000
      --trust-remote-code
      --max-model-len 4096
      --gpu-memory-utilization 0.9
      --dtype auto
```

### 1.6 å¯åŠ¨æœåŠ¡

```bash
# å¯åŠ¨
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# éªŒè¯
curl http://localhost:8000/health
curl http://localhost:8000/v1/models
```

---

## ç¬¬äºŒæ­¥ï¼šmacOS æœ¬åœ°é…ç½® gin

### 2.1 è·å–æœåŠ¡å™¨ IP

```bash
# åœ¨ CentOS æœåŠ¡å™¨ä¸Šæ‰§è¡Œ
ip addr show | grep "inet " | grep -v 127.0.0.1
# è®°å½• IPï¼Œä¾‹å¦‚: 192.168.1.100
```

### 2.2 æµ‹è¯•è¿é€šæ€§

```bash
# åœ¨ macOS ä¸Šæµ‹è¯•
curl http://192.168.1.100:8000/health
curl http://192.168.1.100:8000/v1/models
```

### 2.3 ä¿®æ”¹ gin é…ç½®

ç¼–è¾‘ `server/config.yaml`:

```yaml
# æ·»åŠ  vLLM é…ç½®
vllm:
  enabled: true
  base-url: "http://192.168.1.100:8000"  # æ”¹æˆä½ çš„æœåŠ¡å™¨IP
  model: "Qwen2.5-7B-Instruct-AWQ"
  timeout: 300
  max-tokens: 2000
  temperature: 0.7
```

### 2.4 é‡å¯ gin å®¹å™¨

```bash
# åœ¨ macOS ä¸Š
cd /Users/markxiu/workspace/www/gin-vue-admin
docker-compose restart server
# æˆ–
docker-compose up -d
```

---

## ç¬¬ä¸‰æ­¥ï¼šç¼–å†™ gin é›†æˆä»£ç 

### 3.1 åˆ›å»ºé…ç½®ç»“æ„

`server/config/vllm.go`:

```go
package config

type VLLM struct {
	Enabled     bool    `mapstructure:"enabled" json:"enabled" yaml:"enabled"`
	BaseURL     string  `mapstructure:"base-url" json:"base-url" yaml:"base-url"`
	Model       string  `mapstructure:"model" json:"model" yaml:"model"`
	Timeout     int     `mapstructure:"timeout" json:"timeout" yaml:"timeout"`
	MaxTokens   int     `mapstructure:"max-tokens" json:"max-tokens" yaml:"max-tokens"`
	Temperature float64 `mapstructure:"temperature" json:"temperature" yaml:"temperature"`
}
```

åœ¨ `server/config/config.go` æ·»åŠ :

```go
type Server struct {
    // ... å…¶ä»–é…ç½® ...
    VLLM VLLM `mapstructure:"vllm" json:"vllm" yaml:"vllm"`
}
```

### 3.2 åˆ›å»ºæœåŠ¡å±‚

`server/service/vllm/client.go`:

```go
package vllm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"time"

	"github.com/flipped-aurora/gin-vue-admin/server/global"
)

type VLLMClient struct {
	BaseURL string
	Client  *http.Client
}

func NewVLLMClient() *VLLMClient {
	return &VLLMClient{
		BaseURL: global.GVA_CONFIG.VLLM.BaseURL,
		Client: &http.Client{
			Timeout: time.Duration(global.GVA_CONFIG.VLLM.Timeout) * time.Second,
		},
	}
}

type ChatRequest struct {
	Model       string    `json:"model"`
	Messages    []Message `json:"messages"`
	MaxTokens   int       `json:"max_tokens"`
	Temperature float64   `json:"temperature"`
}

type Message struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type ChatResponse struct {
	Choices []struct {
		Message struct {
			Content string `json:"content"`
		} `json:"message"`
	} `json:"choices"`
	Usage struct {
		PromptTokens     int `json:"prompt_tokens"`
		CompletionTokens int `json:"completion_tokens"`
		TotalTokens      int `json:"total_tokens"`
	} `json:"usage"`
}

func (c *VLLMClient) Chat(ctx context.Context, message string) (string, error) {
	reqBody := ChatRequest{
		Model: global.GVA_CONFIG.VLLM.Model,
		Messages: []Message{
			{Role: "user", Content: message},
		},
		MaxTokens:   global.GVA_CONFIG.VLLM.MaxTokens,
		Temperature: global.GVA_CONFIG.VLLM.Temperature,
	}

	jsonData, _ := json.Marshal(reqBody)
	
	req, err := http.NewRequestWithContext(ctx, "POST",
		c.BaseURL+"/v1/chat/completions",
		bytes.NewReader(jsonData))
	if err != nil {
		return "", err
	}

	req.Header.Set("Content-Type", "application/json")
	
	resp, err := c.Client.Do(req)
	if err != nil {
		return "", err
	}
	defer resp.Body.Close()

	var chatResp ChatResponse
	if err := json.NewDecoder(resp.Body).Decode(&chatResp); err != nil {
		return "", err
	}

	if len(chatResp.Choices) == 0 {
		return "", fmt.Errorf("no response")
	}

	return chatResp.Choices[0].Message.Content, nil
}
```

`server/service/vllm/enter.go`:

```go
package vllm

type ServiceGroup struct {
	VLLMClient
}
```

### 3.3 åˆ›å»º API å±‚

`server/api/v1/vllm/vllm.go`:

```go
package vllm

import (
	"github.com/flipped-aurora/gin-vue-admin/server/global"
	"github.com/flipped-aurora/gin-vue-admin/server/model/common/response"
	"github.com/flipped-aurora/gin-vue-admin/server/service/vllm"
	"github.com/gin-gonic/gin"
	"go.uber.org/zap"
)

type VLLMApi struct{}

type ChatRequest struct {
	Message string `json:"message" binding:"required"`
}

func (v *VLLMApi) Chat(c *gin.Context) {
	var req ChatRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		response.FailWithMessage(err.Error(), c)
		return
	}

	client := vllm.NewVLLMClient()
	reply, err := client.Chat(c.Request.Context(), req.Message)
	if err != nil {
		global.GVA_LOG.Error("vLLMè°ƒç”¨å¤±è´¥!", zap.Error(err))
		response.FailWithMessage("è°ƒç”¨å¤±è´¥: "+err.Error(), c)
		return
	}

	response.OkWithData(gin.H{"reply": reply}, c)
}
```

`server/api/v1/vllm/enter.go`:

```go
package vllm

type ApiGroup struct {
	VLLMApi
}
```

### 3.4 æ³¨å†Œè·¯ç”±

`server/router/vllm/vllm.go`:

```go
package vllm

import (
	v1 "github.com/flipped-aurora/gin-vue-admin/server/api/v1"
	"github.com/gin-gonic/gin"
)

type VLLMRouter struct{}

func (r *VLLMRouter) InitVLLMRouter(Router *gin.RouterGroup) {
	vllmRouter := Router.Group("vllm")
	vllmApi := v1.ApiGroupApp.VLLMApiGroup
	{
		vllmRouter.POST("chat", vllmApi.Chat)
	}
}
```

`server/router/vllm/enter.go`:

```go
package vllm

type RouterGroup struct {
	VLLMRouter
}
```

### 3.5 æ³¨å†Œåˆ°ä¸»ç¨‹åº

ä¿®æ”¹ `server/api/v1/enter.go`:

```go
import (
    // ... å…¶ä»–å¯¼å…¥
    "github.com/flipped-aurora/gin-vue-admin/server/api/v1/vllm"
)

type ApiGroup struct {
    // ... å…¶ä»–
    VLLMApiGroup vllm.ApiGroup
}
```

ä¿®æ”¹ `server/service/enter.go`:

```go
import (
    // ... å…¶ä»–å¯¼å…¥
    "github.com/flipped-aurora/gin-vue-admin/server/service/vllm"
)

type ServiceGroup struct {
    // ... å…¶ä»–
    VLLMServiceGroup vllm.ServiceGroup
}
```

ä¿®æ”¹ `server/router/enter.go`:

```go
import (
    // ... å…¶ä»–å¯¼å…¥
    "github.com/flipped-aurora/gin-vue-admin/server/router/vllm"
)

type RouterGroup struct {
    // ... å…¶ä»–
    VLLM vllm.RouterGroup
}
```

ä¿®æ”¹ `server/initialize/router.go`:

```go
func Routers() *gin.Engine {
    // ... ç°æœ‰ä»£ç  ...
    
    vllmRouter := router.RouterGroupApp.VLLM
    
    // åœ¨ PublicGroup ä¸­æ·»åŠ 
    {
        // ... å…¶ä»–è·¯ç”±
        vllmRouter.InitVLLMRouter(PublicGroup) // vLLM è·¯ç”±
    }
    
    // ... å…¶ä»–ä»£ç 
}
```

---

## ç¬¬å››æ­¥ï¼šæµ‹è¯•

### 4.1 æµ‹è¯• vLLM ç›´è¿

```bash
# æµ‹è¯•æœåŠ¡å™¨
curl -X POST http://192.168.1.100:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "Qwen2.5-7B-Instruct-AWQ",
    "messages": [{"role": "user", "content": "ä½ å¥½"}],
    "max_tokens": 100
  }'
```

### 4.2 æµ‹è¯• gin æ¥å£

```bash
# æµ‹è¯• gin
curl -X POST http://localhost:8888/vllm/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"}'
```

### 4.3 æŸ¥çœ‹æ—¥å¿—

```bash
# CentOS æœåŠ¡å™¨
docker-compose logs -f vllm

# macOS gin
docker-compose logs -f server
```

---

## å¸¸è§é—®é¢˜

### Q1: è¿æ¥è¶…æ—¶

```bash
# æ£€æŸ¥é˜²ç«å¢™
sudo firewall-cmd --add-port=8000/tcp --permanent
sudo firewall-cmd --reload

# æˆ–å…³é—­é˜²ç«å¢™
sudo systemctl stop firewalld
```

### Q2: æ˜¾å­˜ä¸è¶³

```yaml
# ä¿®æ”¹ docker-compose.yml
command: >
  --gpu-memory-utilization 0.8  # é™ä½æ˜¾å­˜ä½¿ç”¨
  --max-model-len 2048          # å‡å°ä¸Šä¸‹æ–‡é•¿åº¦
```

### Q3: æ¨¡å‹ä¸‹è½½æ…¢

```bash
# ä½¿ç”¨å›½å†…é•œåƒ
export HF_ENDPOINT=https://hf-mirror.com
huggingface-cli download Qwen/Qwen2.5-7B-Instruct-AWQ \
  --local-dir ./models/Qwen2.5-7B-Instruct-AWQ
```

---

## å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```bash
# ===== CentOS æœåŠ¡å™¨ =====
# å¯åŠ¨
cd ~/vllm-deploy && docker-compose up -d

# åœæ­¢
docker-compose stop

# é‡å¯
docker-compose restart

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# æŸ¥çœ‹ GPU
nvidia-smi

# ===== macOS æœ¬åœ° =====
# é‡å¯ gin
docker-compose restart server

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f server

# æµ‹è¯•æ¥å£
curl -X POST http://localhost:8888/vllm/chat \
  -H "Content-Type: application/json" \
  -d '{"message": "ä½ å¥½"}'
```

---

## æ€§èƒ½ä¼˜åŒ–

### æœåŠ¡å™¨ç«¯

```yaml
# docker-compose.yml ä¼˜åŒ–å‚æ•°
command: >
  --model /models/Qwen2.5-7B-Instruct-AWQ
  --quantization awq
  --max-model-len 4096
  --gpu-memory-utilization 0.9
  --max-num-seqs 256
  --max-num-batched-tokens 8192
  --enable-prefix-caching
```

### gin ç«¯

```go
// è¿æ¥æ± ä¼˜åŒ–
transport := &http.Transport{
    MaxIdleConns:        100,
    MaxIdleConnsPerHost: 100,
    IdleConnTimeout:     90 * time.Second,
}

Client: &http.Client{
    Transport: transport,
    Timeout:   300 * time.Second,
}
```

---

**å®Œæˆï¼** ğŸ‰

ç°åœ¨ä½ å¯ä»¥ï¼š
1. CentOS æœåŠ¡å™¨è¿è¡Œ vLLM + GPU
2. macOS æœ¬åœ°è¿è¡Œ gin
3. é€šè¿‡ HTTP é€šä¿¡
4. è°ƒç”¨æœ¬åœ°å¤§æ¨¡å‹

